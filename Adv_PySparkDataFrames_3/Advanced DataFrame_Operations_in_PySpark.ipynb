{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb97d3e-acf9-4488-8027-736da79bdcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93aeacc8-82ac-4b09-8961-fe109f7a5727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/25 13:21:08 WARN Utils: Your hostname, bhuvaneshwaran-Latitude-5420, resolves to a loopback address: 127.0.1.1; using 192.168.1.17 instead (on interface wlp0s20f3)\n",
      "25/12/25 13:21:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/25 13:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"Advance_DF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a523ee8-364a-4c14-8a26-cf2a6d78a658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Advance_DF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x74c6af66bc90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf365ef-0bf5-40c6-8c51-c851dcac77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet(r\"/home/bhuvaneshwaran/Desktop/Medium/parquetFiles/Employees.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c37a9e3-1b9e-4b34-bf1c-bf592b6551e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+------------------+--------------------+----------+-------------+\n",
      "| id|            name|          full_name|             address|         json_string|dob_string_format|score| department|salary|        hobbies_list|user_id|            txn_date|         txn_value|              street|      city|        state|\n",
      "+---+----------------+-------------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+------------------+--------------------+----------+-------------+\n",
      "|  1|    Advik Halder|         Janya Bala|H.No. 916, Choksh...|{\"city\": \"Bengalu...|       1974-10-29|   74|      Sales| 95186|       [Photography]|      2|2025-11-29 13:20:...|132.71083639903748|   20/662\\nDua Nagar|Coimbatore|    Rajasthan|\n",
      "|  2|     Omaja Baria|       Manya Bhakta|492, Dass Road, B...|{\"city\": \"Coimbat...|       1978-04-26|   33|    Finance| 54210|[Gaming, Cooking,...|      3|2025-11-28 13:20:...|   829.31653998402|H.No. 913, Chande...| Bengaluru|Uttar Pradesh|\n",
      "|  3|    Unni Iyengar|    Timothy Chaudry|H.No. 511\\nJain G...|{\"city\": \"Mumbai\"...|       1999-07-25|   47|         HR|113058|            [Coding]|      4|2025-12-06 13:20:...| 913.7253175783072|922, Krishnan Circle|Coimbatore|Uttar Pradesh|\n",
      "|  4|Manthan Kulkarni|Thomas Ramakrishnan|47\\nMammen, Coimb...|{\"city\": \"Salem\",...|       1981-08-02|   70|Engineering|149570|[Gaming, Music, R...|      5|2025-11-29 13:20:...| 566.9216358462841|     48/18\\nBir Zila|    Mumbai|     Nagaland|\n",
      "|  5|      Ayaan Rege|  Hiral Rajagopalan|95/81\\nApte Marg,...|{\"city\": \"Coimbat...|       1962-07-13|   80|Engineering| 78937|             [Music]|      6|2025-12-04 13:20:...| 271.9908394269114|      21\\nLanka Ganj|Coimbatore|      Mizoram|\n",
      "+---+----------------+-------------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+------------------+--------------------+----------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5a80f3-1797-4d73-92be-871197de368a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- json_string: string (nullable = true)\n",
      " |-- dob_string_format: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hobbies_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- txn_date: timestamp_ntz (nullable = true)\n",
      " |-- txn_value: double (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()   #Useful for checking nested fields, types, nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cf17cc-4631-4907-9dc3-d94b70031078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'full_name',\n",
       " 'address',\n",
       " 'json_string',\n",
       " 'dob_string_format',\n",
       " 'score',\n",
       " 'department',\n",
       " 'salary',\n",
       " 'hobbies_list',\n",
       " 'user_id',\n",
       " 'txn_date',\n",
       " 'txn_value',\n",
       " 'street',\n",
       " 'city',\n",
       " 'state']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245ca2f-f947-4497-b705-f0d166e56a66",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 1. Advanced String Operations\n",
    "\n",
    "PySpark has a full set of string functions that are extremely useful for data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76599aef-8471-4710-8a6f-e9ae2aeb3233",
   "metadata": {},
   "source": [
    "## âœ” Extract using regex\n",
    "\n",
    "Useful for logs, IDs, URLs, phone numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1385bb-db81-4d7b-8be8-df6507c63422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                name|             address|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|        Advik Halder|H.No. 916, Choksh...|\n",
      "|  2|         Omaja Baria|492, Dass Road, B...|\n",
      "|  3|        Unni Iyengar|H.No. 511\\nJain G...|\n",
      "|  4|    Manthan Kulkarni|47\\nMammen, Coimb...|\n",
      "|  5|          Ayaan Rege|95/81\\nApte Marg,...|\n",
      "|  6|      Aarush Sanghvi|05\\nBawa Street, ...|\n",
      "|  7|      Tristan Sahota|H.No. 349\\nNori M...|\n",
      "|  8|Ikshita Radhakris...|H.No. 54\\nPai Cho...|\n",
      "|  9|      Upadhriti Tata|60/534, Swamy Pat...|\n",
      "| 10|         Arya Sharma|03\\nDubey Ganj, M...|\n",
      "| 11|          Omya Verma|H.No. 85\\nTiwari ...|\n",
      "| 12|         Tejas Kanda|90/51\\nPillai, Be...|\n",
      "| 13|        Jairaj Walla|954\\nZacharia Mar...|\n",
      "| 14|        Jeet Chaudry|H.No. 203\\nPatla ...|\n",
      "| 15|       Arunima Uppal|55/43, Pandey Gan...|\n",
      "| 16|    Upadhriti Sachar|886, Chandra, Sal...|\n",
      "| 17|     Bhanumati Sethi|172\\nSolanki Naga...|\n",
      "| 18|         Darsh Chada|492\\nMukhopadhyay...|\n",
      "| 19|          Rishi Bail|659\\nSodhi Zila, ...|\n",
      "| 20|       Madhavi Ghose|28/73\\nBoase Chow...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"name\",\"address\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffd1da5-520c-4894-8432-42ea6d4b8e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|                name|             address|   pin|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  1|        Advik Halder|H.No. 916, Choksh...|383408|\n",
      "|  2|         Omaja Baria|492, Dass Road, B...|103082|\n",
      "|  3|        Unni Iyengar|H.No. 511\\nJain G...|040655|\n",
      "|  4|    Manthan Kulkarni|47\\nMammen, Coimb...|152655|\n",
      "|  5|          Ayaan Rege|95/81\\nApte Marg,...|752051|\n",
      "|  6|      Aarush Sanghvi|05\\nBawa Street, ...|005744|\n",
      "|  7|      Tristan Sahota|H.No. 349\\nNori M...|678621|\n",
      "|  8|Ikshita Radhakris...|H.No. 54\\nPai Cho...|808613|\n",
      "|  9|      Upadhriti Tata|60/534, Swamy Pat...|621238|\n",
      "| 10|         Arya Sharma|03\\nDubey Ganj, M...|888836|\n",
      "| 11|          Omya Verma|H.No. 85\\nTiwari ...|401088|\n",
      "| 12|         Tejas Kanda|90/51\\nPillai, Be...|184056|\n",
      "| 13|        Jairaj Walla|954\\nZacharia Mar...|219836|\n",
      "| 14|        Jeet Chaudry|H.No. 203\\nPatla ...|228091|\n",
      "| 15|       Arunima Uppal|55/43, Pandey Gan...|498618|\n",
      "| 16|    Upadhriti Sachar|886, Chandra, Sal...|627474|\n",
      "| 17|     Bhanumati Sethi|172\\nSolanki Naga...|357193|\n",
      "| 18|         Darsh Chada|492\\nMukhopadhyay...|506061|\n",
      "| 19|          Rishi Bail|659\\nSodhi Zila, ...|319286|\n",
      "| 20|       Madhavi Ghose|28/73\\nBoase Chow...|663289|\n",
      "+---+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "df = df.withColumn(\"pin\", regexp_extract(\"address\", r\"(\\d{6})\", 1))\n",
    "df.select(\"id\",\"name\",\"address\",\"pin\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad444c-0767-4e42-ab17-d7456a391471",
   "metadata": {},
   "source": [
    "## âœ” Replace patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78eb1164-019d-48af-9ad3-d3b7b40401ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+------+\n",
      "| id|                name|        clean_spaces|             address|   pin|\n",
      "+---+--------------------+--------------------+--------------------+------+\n",
      "|  1|        Advik Halder|         AdvikHalder|H.No. 916, Choksh...|383408|\n",
      "|  2|         Omaja Baria|          OmajaBaria|492, Dass Road, B...|103082|\n",
      "|  3|        Unni Iyengar|         UnniIyengar|H.No. 511\\nJain G...|040655|\n",
      "|  4|    Manthan Kulkarni|     ManthanKulkarni|47\\nMammen, Coimb...|152655|\n",
      "|  5|          Ayaan Rege|           AyaanRege|95/81\\nApte Marg,...|752051|\n",
      "|  6|      Aarush Sanghvi|       AarushSanghvi|05\\nBawa Street, ...|005744|\n",
      "|  7|      Tristan Sahota|       TristanSahota|H.No. 349\\nNori M...|678621|\n",
      "|  8|Ikshita Radhakris...|IkshitaRadhakrishnan|H.No. 54\\nPai Cho...|808613|\n",
      "|  9|      Upadhriti Tata|       UpadhritiTata|60/534, Swamy Pat...|621238|\n",
      "| 10|         Arya Sharma|          AryaSharma|03\\nDubey Ganj, M...|888836|\n",
      "| 11|          Omya Verma|           OmyaVerma|H.No. 85\\nTiwari ...|401088|\n",
      "| 12|         Tejas Kanda|          TejasKanda|90/51\\nPillai, Be...|184056|\n",
      "| 13|        Jairaj Walla|         JairajWalla|954\\nZacharia Mar...|219836|\n",
      "| 14|        Jeet Chaudry|         JeetChaudry|H.No. 203\\nPatla ...|228091|\n",
      "| 15|       Arunima Uppal|        ArunimaUppal|55/43, Pandey Gan...|498618|\n",
      "| 16|    Upadhriti Sachar|     UpadhritiSachar|886, Chandra, Sal...|627474|\n",
      "| 17|     Bhanumati Sethi|      BhanumatiSethi|172\\nSolanki Naga...|357193|\n",
      "| 18|         Darsh Chada|          DarshChada|492\\nMukhopadhyay...|506061|\n",
      "| 19|          Rishi Bail|           RishiBail|659\\nSodhi Zila, ...|319286|\n",
      "| 20|       Madhavi Ghose|        MadhaviGhose|28/73\\nBoase Chow...|663289|\n",
      "+---+--------------------+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df = df.withColumn(\"clean_spaces\", regexp_replace(\"name\", \"[^A-Za-z]\", \"\"))\n",
    "df.select(\"id\",\"name\",\"clean_spaces\",\"address\",\"pin\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b103aa7-de42-440b-aaab-9a7f1928327b",
   "metadata": {},
   "source": [
    "## âœ” Splitting strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7afc02cb-ae97-4ce4-9361-0e5095c483f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.withColumn(\"first_name\", split(\"name\", \" \").getItem(0))\n",
    "df = df.withColumn(\"last_name\", split(\"name\", \" \").getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1759ecd3-ebd3-4452-828d-1645ec486d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-------------+\n",
      "| id|                name|first_name|    last_name|\n",
      "+---+--------------------+----------+-------------+\n",
      "|  1|        Advik Halder|     Advik|       Halder|\n",
      "|  2|         Omaja Baria|     Omaja|        Baria|\n",
      "|  3|        Unni Iyengar|      Unni|      Iyengar|\n",
      "|  4|    Manthan Kulkarni|   Manthan|     Kulkarni|\n",
      "|  5|          Ayaan Rege|     Ayaan|         Rege|\n",
      "|  6|      Aarush Sanghvi|    Aarush|      Sanghvi|\n",
      "|  7|      Tristan Sahota|   Tristan|       Sahota|\n",
      "|  8|Ikshita Radhakris...|   Ikshita|Radhakrishnan|\n",
      "|  9|      Upadhriti Tata| Upadhriti|         Tata|\n",
      "| 10|         Arya Sharma|      Arya|       Sharma|\n",
      "| 11|          Omya Verma|      Omya|        Verma|\n",
      "| 12|         Tejas Kanda|     Tejas|        Kanda|\n",
      "| 13|        Jairaj Walla|    Jairaj|        Walla|\n",
      "| 14|        Jeet Chaudry|      Jeet|      Chaudry|\n",
      "| 15|       Arunima Uppal|   Arunima|        Uppal|\n",
      "| 16|    Upadhriti Sachar| Upadhriti|       Sachar|\n",
      "| 17|     Bhanumati Sethi| Bhanumati|        Sethi|\n",
      "| 18|         Darsh Chada|     Darsh|        Chada|\n",
      "| 19|          Rishi Bail|     Rishi|         Bail|\n",
      "| 20|       Madhavi Ghose|   Madhavi|        Ghose|\n",
      "+---+--------------------+----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"name\",\"first_name\",\"last_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4aa838-d463-4f85-9688-d2a278079665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'full_name',\n",
       " 'address',\n",
       " 'json_string',\n",
       " 'dob_string_format',\n",
       " 'score',\n",
       " 'department',\n",
       " 'salary',\n",
       " 'hobbies_list',\n",
       " 'user_id',\n",
       " 'txn_date',\n",
       " 'txn_value',\n",
       " 'street',\n",
       " 'city',\n",
       " 'state',\n",
       " 'pin',\n",
       " 'clean_spaces',\n",
       " 'first_name',\n",
       " 'last_name']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed8ed09-16b5-4699-bbe8-5b9eaf668e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             address|\n",
      "+--------------------+\n",
      "|H.No. 916, Choksh...|\n",
      "|492, Dass Road, B...|\n",
      "|H.No. 511\\nJain G...|\n",
      "|47\\nMammen, Coimb...|\n",
      "|95/81\\nApte Marg,...|\n",
      "|05\\nBawa Street, ...|\n",
      "|H.No. 349\\nNori M...|\n",
      "|H.No. 54\\nPai Cho...|\n",
      "|60/534, Swamy Pat...|\n",
      "|03\\nDubey Ganj, M...|\n",
      "|H.No. 85\\nTiwari ...|\n",
      "|90/51\\nPillai, Be...|\n",
      "|954\\nZacharia Mar...|\n",
      "|H.No. 203\\nPatla ...|\n",
      "|55/43, Pandey Gan...|\n",
      "|886, Chandra, Sal...|\n",
      "|172\\nSolanki Naga...|\n",
      "|492\\nMukhopadhyay...|\n",
      "|659\\nSodhi Zila, ...|\n",
      "|28/73\\nBoase Chow...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"address\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc917ab-c7c8-4f32-ba88-8ed117c45a2d",
   "metadata": {},
   "source": [
    "## âœ” Combining multiple fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dbe50c5-8b0e-4d23-835b-5a15cf48f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df = df.withColumn(\"full_address\", concat_ws(\", \", \"street\", \"city\", \"state\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d3161d-942b-4a75-b164-e407075a854b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+-----------------+\n",
      "|             address|        full_address|              street|      city|            state|\n",
      "+--------------------+--------------------+--------------------+----------+-----------------+\n",
      "|H.No. 916, Choksh...|20/662\\nDua Nagar...|   20/662\\nDua Nagar|Coimbatore|        Rajasthan|\n",
      "|492, Dass Road, B...|H.No. 913, Chande...|H.No. 913, Chande...| Bengaluru|    Uttar Pradesh|\n",
      "|H.No. 511\\nJain G...|922, Krishnan Cir...|922, Krishnan Circle|Coimbatore|    Uttar Pradesh|\n",
      "|47\\nMammen, Coimb...|48/18\\nBir Zila, ...|     48/18\\nBir Zila|    Mumbai|         Nagaland|\n",
      "|95/81\\nApte Marg,...|21\\nLanka Ganj, C...|      21\\nLanka Ganj|Coimbatore|          Mizoram|\n",
      "|05\\nBawa Street, ...|H.No. 79\\nSachar ...|H.No. 79\\nSachar ...|    Mumbai| Himachal Pradesh|\n",
      "|H.No. 349\\nNori M...|58\\nMall Circle, ...|     58\\nMall Circle|     Salem|            Assam|\n",
      "|H.No. 54\\nPai Cho...|H.No. 76\\nBawa, M...|      H.No. 76\\nBawa|    Mumbai|      Uttarakhand|\n",
      "|60/534, Swamy Pat...|H.No. 939\\nVig Ga...| H.No. 939\\nVig Ganj|     Salem|Arunachal Pradesh|\n",
      "|03\\nDubey Ganj, M...|42/20, Pandey Gan...|  42/20, Pandey Ganj|    Mumbai|        Jharkhand|\n",
      "|H.No. 85\\nTiwari ...|120\\nLuthra, Mumb...|         120\\nLuthra|    Mumbai|          Manipur|\n",
      "|90/51\\nPillai, Be...|72, Tiwari Path, ...|     72, Tiwari Path|   Chennai|          Haryana|\n",
      "|954\\nZacharia Mar...|734, Salvi Road, ...|     734, Salvi Road|     Salem|          Gujarat|\n",
      "|H.No. 203\\nPatla ...|96, Basu, Coimbat...|            96, Basu|Coimbatore|   Andhra Pradesh|\n",
      "|55/43, Pandey Gan...|H.No. 66\\nShenoy,...|    H.No. 66\\nShenoy| Bengaluru|           Kerala|\n",
      "|886, Chandra, Sal...|86, Muni Chowk, S...|      86, Muni Chowk|     Salem|    Uttar Pradesh|\n",
      "|172\\nSolanki Naga...|79/01\\nChopra Mar...|  79/01\\nChopra Marg| Bengaluru|          Tripura|\n",
      "|492\\nMukhopadhyay...|90, Pau Chowk, Co...|       90, Pau Chowk|Coimbatore|        Karnataka|\n",
      "|659\\nSodhi Zila, ...|H.No. 28, Mandal ...|H.No. 28, Mandal ...|Coimbatore|          Tripura|\n",
      "|28/73\\nBoase Chow...|H.No. 92, Bir Cho...| H.No. 92, Bir Chowk|Coimbatore|      West Bengal|\n",
      "+--------------------+--------------------+--------------------+----------+-----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"address\",'full_address',\"street\", \"city\", \"state\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddcb986-38d3-49cf-a291-c7265bf661f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        hobbies_list|\n",
      "+--------------------+\n",
      "|       [Photography]|\n",
      "|[Gaming, Cooking,...|\n",
      "|            [Coding]|\n",
      "|[Gaming, Music, R...|\n",
      "|             [Music]|\n",
      "|[Sports, Farming,...|\n",
      "|[Photography, Spo...|\n",
      "|[Photography, Music]|\n",
      "|[Gaming, Cooking,...|\n",
      "|[Trekking, Cookin...|\n",
      "|[Sports, Cooking,...|\n",
      "|            [Gaming]|\n",
      "|[Music, Farming, ...|\n",
      "|          [Trekking]|\n",
      "|[Farming, Gaming,...|\n",
      "|[Farming, Photogr...|\n",
      "|           [Farming]|\n",
      "|[Cooking, Coding,...|\n",
      "|    [Coding, Gaming]|\n",
      "|[Photography, Rea...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"hobbies_list\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bac00-f253-4d48-a673-6e44558af1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¥ 2. Working with Arrays\n",
    "\n",
    "Array columns are common in logs, events, clickstreams, etc.\n",
    "\n",
    "Array Operations\n",
    "explode(col): This is a powerful transformation.\n",
    "\n",
    "It takes a list (like [Gaming, Coding]) and turns it into two separate rowsâ€”one for Gaming and one for Coding. This makes it easier to count or filter specific hobbies.\n",
    "\n",
    "array_contains(col, value): A filter tool.\n",
    "\n",
    "It checks if a specific value (like \"spark\") exists inside the array column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb359793-b2d3-4b7c-a375-c5cbf47ff5b1",
   "metadata": {},
   "source": [
    "## âœ” Explode array into multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffea005f-5625-4c58-9c3b-a92ebb62acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|current_user()|        col|\n",
      "+--------------+-----------+\n",
      "|bhuvaneshwaran|Photography|\n",
      "|bhuvaneshwaran|     Gaming|\n",
      "|bhuvaneshwaran|    Cooking|\n",
      "|bhuvaneshwaran|Photography|\n",
      "|bhuvaneshwaran|    Reading|\n",
      "|bhuvaneshwaran|     Coding|\n",
      "|bhuvaneshwaran|     Gaming|\n",
      "|bhuvaneshwaran|      Music|\n",
      "|bhuvaneshwaran|    Reading|\n",
      "|bhuvaneshwaran|      Music|\n",
      "|bhuvaneshwaran|     Sports|\n",
      "|bhuvaneshwaran|    Farming|\n",
      "|bhuvaneshwaran|     Gaming|\n",
      "|bhuvaneshwaran|Photography|\n",
      "|bhuvaneshwaran|     Sports|\n",
      "|bhuvaneshwaran|    Farming|\n",
      "|bhuvaneshwaran|Photography|\n",
      "|bhuvaneshwaran|      Music|\n",
      "|bhuvaneshwaran|     Gaming|\n",
      "|bhuvaneshwaran|    Cooking|\n",
      "+--------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(\"user\", explode(\"hobbies_list\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf946103-a266-4ec6-874d-a13b5ca370e8",
   "metadata": {},
   "source": [
    "## âœ” Count array items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a642136-7a7e-4fb0-acab-bdaf802a96e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                name|size(hobbies_list)|\n",
      "+--------------------+------------------+\n",
      "|        Advik Halder|                 1|\n",
      "|         Omaja Baria|                 4|\n",
      "|        Unni Iyengar|                 1|\n",
      "|    Manthan Kulkarni|                 3|\n",
      "|          Ayaan Rege|                 1|\n",
      "|      Aarush Sanghvi|                 3|\n",
      "|      Tristan Sahota|                 3|\n",
      "|Ikshita Radhakris...|                 2|\n",
      "|      Upadhriti Tata|                 4|\n",
      "|         Arya Sharma|                 4|\n",
      "|          Omya Verma|                 3|\n",
      "|         Tejas Kanda|                 1|\n",
      "|        Jairaj Walla|                 3|\n",
      "|        Jeet Chaudry|                 1|\n",
      "|       Arunima Uppal|                 3|\n",
      "|    Upadhriti Sachar|                 2|\n",
      "|     Bhanumati Sethi|                 1|\n",
      "|         Darsh Chada|                 4|\n",
      "|          Rishi Bail|                 2|\n",
      "|       Madhavi Ghose|                 2|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(\"name\", size(\"hobbies_list\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d18b5373-05b1-41af-86ba-cfb36fa370e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- json_string: string (nullable = true)\n",
      " |-- dob_string_format: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hobbies_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- txn_date: timestamp_ntz (nullable = true)\n",
      " |-- txn_value: double (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- pin: string (nullable = true)\n",
      " |-- clean_spaces: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- full_address: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a8623-a032-4f2e-a3db-f9324d9cee33",
   "metadata": {},
   "source": [
    "## âœ” Check if an item exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68b7fdbc-f200-4f9a-b3b4-a1822cab987b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+-----------+-----------------+-----+----------+------+------------+-------+--------+---------+------+----+-----+---+------------+----------+---------+------------+\n",
      "| id|name|full_name|address|json_string|dob_string_format|score|department|salary|hobbies_list|user_id|txn_date|txn_value|street|city|state|pin|clean_spaces|first_name|last_name|full_address|\n",
      "+---+----+---------+-------+-----------+-----------------+-----+----------+------+------------+-------+--------+---------+------+----+-----+---+------------+----------+---------+------------+\n",
      "+---+----+---------+-------+-----------+-----------------+-----+----------+------+------------+-------+--------+---------+------+----+-----+---+------------+----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.filter(array_contains(\"hobbies_list\", \"spark\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a93a4bb7-59dc-4b05-98d1-13d91cb0e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         json_string|\n",
      "+--------------------+\n",
      "|{\"city\": \"Bengalu...|\n",
      "|{\"city\": \"Coimbat...|\n",
      "|{\"city\": \"Mumbai\"...|\n",
      "|{\"city\": \"Salem\",...|\n",
      "|{\"city\": \"Coimbat...|\n",
      "|{\"city\": \"Mumbai\"...|\n",
      "|{\"city\": \"Bengalu...|\n",
      "|{\"city\": \"Mumbai\"...|\n",
      "|{\"city\": \"Chennai...|\n",
      "|{\"city\": \"Chennai...|\n",
      "|{\"city\": \"Coimbat...|\n",
      "|{\"city\": \"Coimbat...|\n",
      "|{\"city\": \"Chennai...|\n",
      "|{\"city\": \"Chennai...|\n",
      "|{\"city\": \"Coimbat...|\n",
      "|{\"city\": \"Mumbai\"...|\n",
      "|{\"city\": \"Salem\",...|\n",
      "|{\"city\": \"Coimbat...|\n",
      "|{\"city\": \"Chennai...|\n",
      "|{\"city\": \"Chennai...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"json_string\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd09d9-5b95-48ff-99d0-b7ff8d21171d",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 3. Handling JSON Columns\n",
    "\n",
    "Real-world datasets often contain nested JSON in a single column.\n",
    "\n",
    "JSON Operations\n",
    "\n",
    "from_json(col, schema): Converts a \"string\" that looks like JSON into a structured Spark object.\n",
    "\n",
    "schema: This is required. You must tell Spark the structure (e.g., city is a String) so it knows how to parse the text.\n",
    "\n",
    "to_json(col): The opposite of from_json. It turns a complex structure back into a simple text string, which is useful for saving data to certain file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75e8b50c-d03e-4249-82f2-ae4e17b839c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"zipcode\", StringType())\n",
    "])\n",
    "\n",
    "df = df.withColumn(\"json_data\", from_json(\"json_string\", schema))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b49fb3b-a25a-4990-bc49-f9e078d4cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           json_data|\n",
      "+--------------------+\n",
      "| {Bengaluru, 724094}|\n",
      "|{Coimbatore, 494894}|\n",
      "|    {Mumbai, 334596}|\n",
      "|     {Salem, 041029}|\n",
      "|{Coimbatore, 393996}|\n",
      "|    {Mumbai, 995656}|\n",
      "| {Bengaluru, 607197}|\n",
      "|    {Mumbai, 512139}|\n",
      "|   {Chennai, 122272}|\n",
      "|   {Chennai, 559663}|\n",
      "|{Coimbatore, 036996}|\n",
      "|{Coimbatore, 754531}|\n",
      "|   {Chennai, 831143}|\n",
      "|   {Chennai, 061300}|\n",
      "|{Coimbatore, 639831}|\n",
      "|    {Mumbai, 980755}|\n",
      "|     {Salem, 346144}|\n",
      "|{Coimbatore, 438812}|\n",
      "|   {Chennai, 469240}|\n",
      "|   {Chennai, 254687}|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"json_data\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7520c-9daa-471f-b547-53469a8d9924",
   "metadata": {},
   "source": [
    "## âœ” Extract fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afb81490-ddeb-40f7-abe0-0b575205fc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      city|zipcode|\n",
      "+----------+-------+\n",
      "| Bengaluru| 724094|\n",
      "|Coimbatore| 494894|\n",
      "|    Mumbai| 334596|\n",
      "|     Salem| 041029|\n",
      "|Coimbatore| 393996|\n",
      "|    Mumbai| 995656|\n",
      "| Bengaluru| 607197|\n",
      "|    Mumbai| 512139|\n",
      "|   Chennai| 122272|\n",
      "|   Chennai| 559663|\n",
      "|Coimbatore| 036996|\n",
      "|Coimbatore| 754531|\n",
      "|   Chennai| 831143|\n",
      "|   Chennai| 061300|\n",
      "|Coimbatore| 639831|\n",
      "|    Mumbai| 980755|\n",
      "|     Salem| 346144|\n",
      "|Coimbatore| 438812|\n",
      "|   Chennai| 469240|\n",
      "|   Chennai| 254687|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"json_data.city\", \"json_data.zipcode\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9feb9d-3563-4e9b-a501-d6524ec9d300",
   "metadata": {},
   "source": [
    "## âœ” Convert struct â†’ JSON text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dd97eac-3ea4-453a-8834-379614f3b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+------------------+--------------------+----------+-----------------+------+--------------------+----------+-------------+--------------------+--------------------+\n",
      "| id|                name|          full_name|             address|         json_string|dob_string_format|score| department|salary|        hobbies_list|user_id|            txn_date|         txn_value|              street|      city|            state|   pin|        clean_spaces|first_name|    last_name|        full_address|           json_data|\n",
      "+---+--------------------+-------------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+------------------+--------------------+----------+-----------------+------+--------------------+----------+-------------+--------------------+--------------------+\n",
      "|  1|        Advik Halder|         Janya Bala|H.No. 916, Choksh...|{\"city\":\"Bengalur...|       1974-10-29|   74|      Sales| 95186|       [Photography]|      2|2025-11-29 13:20:...|132.71083639903748|   20/662\\nDua Nagar|Coimbatore|        Rajasthan|383408|         AdvikHalder|     Advik|       Halder|20/662\\nDua Nagar...| {Bengaluru, 724094}|\n",
      "|  2|         Omaja Baria|       Manya Bhakta|492, Dass Road, B...|{\"city\":\"Coimbato...|       1978-04-26|   33|    Finance| 54210|[Gaming, Cooking,...|      3|2025-11-28 13:20:...|   829.31653998402|H.No. 913, Chande...| Bengaluru|    Uttar Pradesh|103082|          OmajaBaria|     Omaja|        Baria|H.No. 913, Chande...|{Coimbatore, 494894}|\n",
      "|  3|        Unni Iyengar|    Timothy Chaudry|H.No. 511\\nJain G...|{\"city\":\"Mumbai\",...|       1999-07-25|   47|         HR|113058|            [Coding]|      4|2025-12-06 13:20:...| 913.7253175783072|922, Krishnan Circle|Coimbatore|    Uttar Pradesh|040655|         UnniIyengar|      Unni|      Iyengar|922, Krishnan Cir...|    {Mumbai, 334596}|\n",
      "|  4|    Manthan Kulkarni|Thomas Ramakrishnan|47\\nMammen, Coimb...|{\"city\":\"Salem\",\"...|       1981-08-02|   70|Engineering|149570|[Gaming, Music, R...|      5|2025-11-29 13:20:...| 566.9216358462841|     48/18\\nBir Zila|    Mumbai|         Nagaland|152655|     ManthanKulkarni|   Manthan|     Kulkarni|48/18\\nBir Zila, ...|     {Salem, 041029}|\n",
      "|  5|          Ayaan Rege|  Hiral Rajagopalan|95/81\\nApte Marg,...|{\"city\":\"Coimbato...|       1962-07-13|   80|Engineering| 78937|             [Music]|      6|2025-12-04 13:20:...| 271.9908394269114|      21\\nLanka Ganj|Coimbatore|          Mizoram|752051|           AyaanRege|     Ayaan|         Rege|21\\nLanka Ganj, C...|{Coimbatore, 393996}|\n",
      "|  6|      Aarush Sanghvi|       Nitara Misra|05\\nBawa Street, ...|{\"city\":\"Mumbai\",...|       1975-05-04|   71|      Sales|145117|[Sports, Farming,...|      7|2025-12-15 13:20:...|500.23970115839415|H.No. 79\\nSachar ...|    Mumbai| Himachal Pradesh|005744|       AarushSanghvi|    Aarush|      Sanghvi|H.No. 79\\nSachar ...|    {Mumbai, 995656}|\n",
      "|  7|      Tristan Sahota|        Abdul Agate|H.No. 349\\nNori M...|{\"city\":\"Bengalur...|       1989-01-22|   33|      Sales| 34016|[Photography, Spo...|      8|2025-11-25 13:20:...|  540.363762364439|     58\\nMall Circle|     Salem|            Assam|678621|       TristanSahota|   Tristan|       Sahota|58\\nMall Circle, ...| {Bengaluru, 607197}|\n",
      "|  8|Ikshita Radhakris...|        Mason Dugar|H.No. 54\\nPai Cho...|{\"city\":\"Mumbai\",...|       1964-05-01|   63|    Finance|136837|[Photography, Music]|      9|2025-12-19 13:20:...| 814.9417332484913|      H.No. 76\\nBawa|    Mumbai|      Uttarakhand|808613|IkshitaRadhakrishnan|   Ikshita|Radhakrishnan|H.No. 76\\nBawa, M...|    {Mumbai, 512139}|\n",
      "|  9|      Upadhriti Tata|Upadhriti Prabhakar|60/534, Swamy Pat...|{\"city\":\"Chennai\"...|       2007-11-06|   90|    Finance|123045|[Gaming, Cooking,...|     10|2025-12-16 13:20:...| 87.03625781156568| H.No. 939\\nVig Ganj|     Salem|Arunachal Pradesh|621238|       UpadhritiTata| Upadhriti|         Tata|H.No. 939\\nVig Ga...|   {Chennai, 122272}|\n",
      "| 10|         Arya Sharma|Nitesh Mukhopadhyay|03\\nDubey Ganj, M...|{\"city\":\"Chennai\"...|       1973-02-28|   73|Engineering| 36329|[Trekking, Cookin...|     11|2025-12-18 13:20:...|  72.5659838971842|  42/20, Pandey Ganj|    Mumbai|        Jharkhand|888836|          AryaSharma|      Arya|       Sharma|42/20, Pandey Gan...|   {Chennai, 559663}|\n",
      "| 11|          Omya Verma|        Charvi Dave|H.No. 85\\nTiwari ...|{\"city\":\"Coimbato...|       1962-01-13|   68|Engineering| 76844|[Sports, Cooking,...|     12|2025-12-09 13:20:...| 281.7206309434518|         120\\nLuthra|    Mumbai|          Manipur|401088|           OmyaVerma|      Omya|        Verma|120\\nLuthra, Mumb...|{Coimbatore, 036996}|\n",
      "| 12|         Tejas Kanda|      Aashi Kadakia|90/51\\nPillai, Be...|{\"city\":\"Coimbato...|       1963-01-01|   43|  Marketing|121007|            [Gaming]|     13|2025-12-03 13:20:...|230.00589320909074|     72, Tiwari Path|   Chennai|          Haryana|184056|          TejasKanda|     Tejas|        Kanda|72, Tiwari Path, ...|{Coimbatore, 754531}|\n",
      "| 13|        Jairaj Walla|    Anthony Panchal|954\\nZacharia Mar...|{\"city\":\"Chennai\"...|       2006-03-21|   33|    Finance|114744|[Music, Farming, ...|     14|2025-12-03 13:20:...| 537.9100470797238|     734, Salvi Road|     Salem|          Gujarat|219836|         JairajWalla|    Jairaj|        Walla|734, Salvi Road, ...|   {Chennai, 831143}|\n",
      "| 14|        Jeet Chaudry|       Vivaan Anand|H.No. 203\\nPatla ...|{\"city\":\"Chennai\"...|       1983-09-12|   50|  Marketing| 76061|          [Trekking]|     15|2025-12-10 13:20:...| 849.8031811335358|            96, Basu|Coimbatore|   Andhra Pradesh|228091|         JeetChaudry|      Jeet|      Chaudry|96, Basu, Coimbat...|   {Chennai, 061300}|\n",
      "| 15|       Arunima Uppal|     Shivani Tailor|55/43, Pandey Gan...|{\"city\":\"Coimbato...|       1981-04-20|   82|  Marketing|100911|[Farming, Gaming,...|     16|2025-12-14 13:20:...| 968.1346253002006|    H.No. 66\\nShenoy| Bengaluru|           Kerala|498618|        ArunimaUppal|   Arunima|        Uppal|H.No. 66\\nShenoy,...|{Coimbatore, 639831}|\n",
      "| 16|    Upadhriti Sachar|        Nathan Loke|886, Chandra, Sal...|{\"city\":\"Mumbai\",...|       2005-08-12|   92|         HR| 99075|[Farming, Photogr...|     17|2025-12-03 13:20:...|  937.095377190446|      86, Muni Chowk|     Salem|    Uttar Pradesh|627474|     UpadhritiSachar| Upadhriti|       Sachar|86, Muni Chowk, S...|    {Mumbai, 980755}|\n",
      "| 17|     Bhanumati Sethi|         Wazir Gera|172\\nSolanki Naga...|{\"city\":\"Salem\",\"...|       1994-08-18|   80|  Marketing|119373|           [Farming]|     18|2025-12-02 13:20:...|  570.476390311164|  79/01\\nChopra Marg| Bengaluru|          Tripura|357193|      BhanumatiSethi| Bhanumati|        Sethi|79/01\\nChopra Mar...|     {Salem, 346144}|\n",
      "| 18|         Darsh Chada|       Zaitra Handa|492\\nMukhopadhyay...|{\"city\":\"Coimbato...|       1993-11-05|   75|      Sales|126499|[Cooking, Coding,...|     19|2025-12-13 13:20:...|310.36500086880557|       90, Pau Chowk|Coimbatore|        Karnataka|506061|          DarshChada|     Darsh|        Chada|90, Pau Chowk, Co...|{Coimbatore, 438812}|\n",
      "| 19|          Rishi Bail|       Tristan Iyer|659\\nSodhi Zila, ...|{\"city\":\"Chennai\"...|       2007-10-02|   64|    Finance|105467|    [Coding, Gaming]|     20|2025-12-19 13:20:...| 80.59658796100761|H.No. 28, Mandal ...|Coimbatore|          Tripura|319286|           RishiBail|     Rishi|         Bail|H.No. 28, Mandal ...|   {Chennai, 469240}|\n",
      "| 20|       Madhavi Ghose|       Samesh Bains|28/73\\nBoase Chow...|{\"city\":\"Chennai\"...|       2002-02-17|   70|         HR|117193|[Photography, Rea...|     21|2025-12-23 13:20:...| 557.3296074747828| H.No. 92, Bir Chowk|Coimbatore|      West Bengal|663289|        MadhaviGhose|   Madhavi|        Ghose|H.No. 92, Bir Cho...|   {Chennai, 254687}|\n",
      "+---+--------------------+-------------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+------------------+--------------------+----------+-----------------+------+--------------------+----------+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df = df.withColumn(\"json_string\", to_json(\"json_data\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0953dad-e4f8-4541-9268-25a4f17dca66",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 4. Date & Timestamp Functions\n",
    "\n",
    "PySpark automatically handles dates, but advanced transformations are extremely useful for time-series work.\n",
    "\n",
    "PySpark is very strict with dates. Converting them correctly is essential for \"Time Series\" analysis.\n",
    "\n",
    "to_date(col, format): Converts a string to a real Date type.\n",
    "\n",
    "format: \"yyyy-MM-dd\". If your string is 29-11-2025, you would change this to \"dd-MM-yyyy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1b2cf2c-15d3-4270-874d-bf6d7a2d0a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- json_string: string (nullable = true)\n",
      " |-- dob_string_format: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hobbies_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- txn_date: timestamp_ntz (nullable = true)\n",
      " |-- txn_value: double (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- pin: string (nullable = true)\n",
      " |-- clean_spaces: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- full_address: string (nullable = false)\n",
      " |-- json_data: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9304b6f-71a7-467f-bb9b-b85411291a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dob_string_format|\n",
      "+-----------------+\n",
      "|       1974-10-29|\n",
      "|       1978-04-26|\n",
      "|       1999-07-25|\n",
      "|       1981-08-02|\n",
      "|       1962-07-13|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dob_string_format\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78582d54-d993-40c8-b20b-edd527ef8c40",
   "metadata": {},
   "source": [
    "## âœ” Convert string â†’ date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c804793d-cfc1-4813-9626-153a3727e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       dob|\n",
      "+----------+\n",
      "|1974-10-29|\n",
      "|1978-04-26|\n",
      "|1999-07-25|\n",
      "|1981-08-02|\n",
      "|1962-07-13|\n",
      "+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn(\"dob\", to_date(\"dob_string_format\", \"yyyy-MM-dd\"))\n",
    "df.select(\"dob\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58f0aaa3-96e1-42df-8ab3-66b7abaf1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- json_string: string (nullable = true)\n",
      " |-- dob_string_format: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hobbies_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- txn_date: timestamp_ntz (nullable = true)\n",
      " |-- txn_value: double (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- pin: string (nullable = true)\n",
      " |-- clean_spaces: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- full_address: string (nullable = false)\n",
      " |-- json_data: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c7333-9a56-41a0-97a2-a993b413e22f",
   "metadata": {},
   "source": [
    "## âœ” Extract year/month/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48614740-3380-46c1-91b5-62781105b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|year(dob)|month(dob)|dayofweek(dob)|\n",
      "+---------+----------+--------------+\n",
      "|     1974|        10|             3|\n",
      "|     1978|         4|             4|\n",
      "|     1999|         7|             1|\n",
      "|     1981|         8|             1|\n",
      "|     1962|         7|             6|\n",
      "+---------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofweek\n",
    "\n",
    "df.select(year(\"dob\"), month(\"dob\"), dayofweek(\"dob\")).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17370727-95fd-4f08-9736-d31958bc97a3",
   "metadata": {},
   "source": [
    "## âœ” Calculate age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720eedec-1f13-4cb1-a23a-4f2705dfd073",
   "metadata": {},
   "source": [
    "datediff(end, start): \n",
    "\n",
    "Returns the number of days between two dates.\n",
    "\n",
    "In your notebook, you divide the result by 365 to convert \"days\" into \"years\" to calculate age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "505fc9c2-7f05-4c0b-b9ac-1084d25db8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,current_date\n",
    "\n",
    "df = df.withColumn(\"age\", datediff(current_date(), \"dob\") / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80eaa0c4-ea83-40e4-9e2f-8dd6a7b0bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               age|\n",
      "+------------------+\n",
      "| 51.19178082191781|\n",
      "|  47.6986301369863|\n",
      "|26.438356164383563|\n",
      "| 44.42739726027397|\n",
      "| 63.49589041095891|\n",
      "+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"age\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21be7c-f522-49c5-8ca4-741e756bd3aa",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 5. Advanced Conditional Logic\n",
    "\n",
    "You already know whenâ€¦ but hereâ€™s how to use multiple conditions the right way.\n",
    "\n",
    "## âœ” If-Else ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50547337-ebf5-41c9-b69e-79095c76ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"rating\",\n",
    "    when(df.score >= 90, \"Excellent\")\n",
    "    .when(df.score >= 70, \"Good\")\n",
    "    .when(df.score >= 50, \"Average\")\n",
    "    .otherwise(\"Poor\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a07af69c-c8a0-4dc0-b1fa-462090a9243f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|score|   rating|\n",
      "+-----+---------+\n",
      "|   74|     Good|\n",
      "|   33|     Poor|\n",
      "|   47|     Poor|\n",
      "|   70|     Good|\n",
      "|   80|     Good|\n",
      "|   71|     Good|\n",
      "|   33|     Poor|\n",
      "|   63|  Average|\n",
      "|   90|Excellent|\n",
      "|   73|     Good|\n",
      "|   68|  Average|\n",
      "|   43|     Poor|\n",
      "|   33|     Poor|\n",
      "|   50|  Average|\n",
      "|   82|     Good|\n",
      "|   92|Excellent|\n",
      "|   80|     Good|\n",
      "|   75|     Good|\n",
      "|   64|  Average|\n",
      "|   70|     Good|\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"score\",\"rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ba0f293-21ea-4367-9a61-e8aceec7962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      city|\n",
      "+----------+\n",
      "|Coimbatore|\n",
      "| Bengaluru|\n",
      "|Coimbatore|\n",
      "|    Mumbai|\n",
      "|Coimbatore|\n",
      "|    Mumbai|\n",
      "|     Salem|\n",
      "|    Mumbai|\n",
      "|     Salem|\n",
      "|    Mumbai|\n",
      "|    Mumbai|\n",
      "|   Chennai|\n",
      "|     Salem|\n",
      "|Coimbatore|\n",
      "| Bengaluru|\n",
      "|     Salem|\n",
      "| Bengaluru|\n",
      "|Coimbatore|\n",
      "|Coimbatore|\n",
      "|Coimbatore|\n",
      "+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"city\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb54ca-c969-4c3e-be34-561e2e6b927a",
   "metadata": {},
   "source": [
    "## âœ” Complex business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80a707c4-15f0-4e69-ac93-b8bb1158d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"category\",\n",
    "    when((df.age > 18) & (df.city == \"Chennai\"), \"Adult-Chennai\")\n",
    "    .otherwise(\"Others\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d886ebc-d33a-48b6-a180-3fc7a29040d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|      city|     category|\n",
      "+----------+-------------+\n",
      "|Coimbatore|       Others|\n",
      "| Bengaluru|       Others|\n",
      "|Coimbatore|       Others|\n",
      "|    Mumbai|       Others|\n",
      "|Coimbatore|       Others|\n",
      "|    Mumbai|       Others|\n",
      "|     Salem|       Others|\n",
      "|    Mumbai|       Others|\n",
      "|     Salem|       Others|\n",
      "|    Mumbai|       Others|\n",
      "|    Mumbai|       Others|\n",
      "|   Chennai|Adult-Chennai|\n",
      "|     Salem|       Others|\n",
      "|Coimbatore|       Others|\n",
      "| Bengaluru|       Others|\n",
      "|     Salem|       Others|\n",
      "| Bengaluru|       Others|\n",
      "|Coimbatore|       Others|\n",
      "|Coimbatore|       Others|\n",
      "|Coimbatore|       Others|\n",
      "+----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"city\",\"category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b360954-0dd7-4116-bcec-6537cd25d2f9",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 6. Window Functions (Expert-Level Spark Feature)\n",
    "\n",
    "Window functions help you perform:\n",
    "\n",
    "    * Ranking\n",
    "\n",
    "    * Moving averages\n",
    "\n",
    "    * Cumulative sums\n",
    "\n",
    "    * Partition-based calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dcc9d-b8f8-4807-83c4-435f157978e7",
   "metadata": {},
   "source": [
    "## âœ” Ranking Example\n",
    "\n",
    "ðŸªŸ 4. Window Functions (The \"Expert\" Tool)\n",
    "\n",
    "Window functions allow you to calculate values across a group of rows without \"collapsing\" them like a groupBy does.\n",
    "\n",
    "    * Window.partitionBy(col): Defines the group. For example, group by \"department\" to rank people within their own teams.\n",
    "    \n",
    "    * orderBy(col): Defines the order within that group (e.g., sort by salary to find the highest paid).\n",
    "    \n",
    "    * rowsBetween(start, end): Used for moving averages.\n",
    "    \n",
    "        * (-2, 0): This tells Spark to look at the 2 previous rows and the current row to calculate the average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "201ebe8f-b850-42c8-8842-98777a1e3b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+\n",
      "| department|salary|rank|\n",
      "+-----------+------+----+\n",
      "|Engineering|149960|   1|\n",
      "|Engineering|149878|   2|\n",
      "|Engineering|149862|   3|\n",
      "|Engineering|149840|   4|\n",
      "|Engineering|149783|   5|\n",
      "|Engineering|149769|   6|\n",
      "|Engineering|149763|   7|\n",
      "|Engineering|149750|   8|\n",
      "|Engineering|149666|   9|\n",
      "|Engineering|149626|  10|\n",
      "|Engineering|149570|  11|\n",
      "|Engineering|149565|  12|\n",
      "|Engineering|149537|  13|\n",
      "|Engineering|149384|  14|\n",
      "|Engineering|149369|  15|\n",
      "|Engineering|149340|  16|\n",
      "|Engineering|149339|  17|\n",
      "|Engineering|149329|  18|\n",
      "|Engineering|149312|  19|\n",
      "|Engineering|149267|  20|\n",
      "+-----------+------+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "w = Window.partitionBy(\"department\").orderBy(df.salary.desc())\n",
    "\n",
    "df.withColumn(\"rank\", row_number().over(w)).select(\"department\",\"salary\",\"rank\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "901c2590-54e6-4c35-a0f5-886d8883c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- json_string: string (nullable = true)\n",
      " |-- dob_string_format: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hobbies_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- txn_date: timestamp_ntz (nullable = true)\n",
      " |-- txn_value: double (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- pin: string (nullable = true)\n",
      " |-- clean_spaces: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- full_address: string (nullable = false)\n",
      " |-- json_data: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- rating: string (nullable = false)\n",
      " |-- category: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cef12-af67-451a-8ac4-e51505d726b2",
   "metadata": {},
   "source": [
    "## âœ” Moving average example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0395e73-b172-444c-b10f-d5c364f95565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            txn_date|         txn_value|        moving_avg|\n",
      "+--------------------+------------------+------------------+\n",
      "|2025-11-25 13:20:...|  540.363762364439|  540.363762364439|\n",
      "|2025-11-25 13:20:...| 589.4899148285357| 564.9268385964874|\n",
      "|2025-11-25 13:20:...| 40.43921437592683| 390.0976305229672|\n",
      "|2025-11-25 13:20:...|332.70184686559713|320.87699202335324|\n",
      "|2025-11-25 13:20:...| 707.0658935131419|360.06898491822193|\n",
      "|2025-11-25 13:20:...| 800.6605250170453| 613.4760884652615|\n",
      "|2025-11-25 13:20:...| 679.1531191622652| 728.9598458974841|\n",
      "|2025-11-25 13:20:...| 411.2931107134427| 630.3689182975844|\n",
      "|2025-11-25 13:20:...| 806.1203075702552| 632.1888458153211|\n",
      "|2025-11-25 13:20:...| 236.5461646161805|484.65319429995947|\n",
      "|2025-11-25 13:20:...| 999.8263220997602|  680.830931428732|\n",
      "|2025-11-25 13:20:...| 907.9984994100212| 714.7903287086541|\n",
      "|2025-11-25 13:20:...| 948.0847027472071| 951.9698414189961|\n",
      "|2025-11-25 13:20:...|390.17078283680496| 748.7513283313443|\n",
      "|2025-11-25 13:20:...| 48.50742837138489|  462.254304651799|\n",
      "|2025-11-25 13:20:...| 900.4108494642829|446.36302022415765|\n",
      "|2025-11-25 13:20:...| 461.5811951039554| 470.1664909798744|\n",
      "|2025-11-25 13:20:...| 807.1555639449762| 723.0492028377381|\n",
      "|2025-11-25 13:20:...|341.26959062289535| 536.6687832239423|\n",
      "|2025-11-25 13:20:...| 517.7295223488752| 555.3848923055822|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/25 17:18:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/25 17:18:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/25 17:18:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/25 17:18:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/25 17:18:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "w = Window.partitionBy(\"user\").orderBy(\"txn_date\").rowsBetween(-2, 0)\n",
    "\n",
    "df.withColumn(\"moving_avg\", avg(\"txn_value\").over(w)).select(\"txn_date\",\"txn_value\",\"moving_avg\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886d62b-6920-4611-85b7-ac3829c28bf4",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 7. Repartitioning & Coalescing (Performance)\n",
    "\n",
    "âš¡ 5. Performance & Optimization\n",
    "    These commands don't change your data, but they change how Spark runs the job.\n",
    "    \n",
    "    * repartition(n): Increases or redistributes data into n files. Use this if you have a massive dataset and want to use more of your CPU cores.\n",
    "    \n",
    "    * coalesce(n): Reduces the number of partitions. This is much faster than repartition because it doesn't move all the data around (no \"shuffle\").\n",
    "    \n",
    "    * cache() / persist(): Stores the DataFrame in the computer's memory (RAM).\n",
    "    \n",
    "        * Why? If you are going to use df for five different charts, cache() it so Spark doesn't have to re-read the original file five times.\n",
    "    \n",
    "    * broadcast(small_df): Used during a Join.\n",
    "\n",
    "        * It sends a tiny table to every worker node. This prevents Spark from moving the \"Big Table,\" making joins significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa219f3d-fc59-46bb-839e-e33605d8bf96",
   "metadata": {},
   "source": [
    "## âœ” Increase partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0543cf41-daf6-48ac-81c1-497cd20e308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429d8be-8043-4778-af5c-b63da209cfdf",
   "metadata": {},
   "source": [
    "## âœ” Reduce partitions (faster than repartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4637bb6e-d89c-41fe-8991-4b5ebfe7ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86325df3-bf3f-4da5-b460-7fc38295b13c",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 8. Caching & Persistence\n",
    "\n",
    "Cache when you will reuse the same DataFrame multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32d55c0b-1b98-42be-979b-8e83b70421cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/25 17:22:06 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, full_name: string, address: string, json_string: string, dob_string_format: string, score: bigint, department: string, salary: bigint, hobbies_list: array<string>, user_id: bigint, txn_date: timestamp_ntz, txn_value: double, street: string, city: string, state: string, pin: string, clean_spaces: string, first_name: string, last_name: string, full_address: string, json_data: struct<city:string,zipcode:string>, dob: date, age: double, rating: string, category: string]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e559f674-d9f2-43c9-be0f-ab6d3e427cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e1b86-16e2-4f30-b8c5-699e6ecf96d7",
   "metadata": {},
   "source": [
    "Alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "646260af-e32b-4472-a275-dbb8069c18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/25 17:22:40 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, full_name: string, address: string, json_string: string, dob_string_format: string, score: bigint, department: string, salary: bigint, hobbies_list: array<string>, user_id: bigint, txn_date: timestamp_ntz, txn_value: double, street: string, city: string, state: string, pin: string, clean_spaces: string, first_name: string, last_name: string, full_address: string, json_data: struct<city:string,zipcode:string>, dob: date, age: double, rating: string, category: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e01db-a455-48c6-8538-31586492a85e",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 9. Broadcast Joins (High Performance)\n",
    "\n",
    "For small lookup tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2fb1ab7c-21ec-4885-8dfb-5ad7337eebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   id|exp|\n",
      "+-----+---+\n",
      "| 2818|  5|\n",
      "|10787|  7|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data=[\n",
    "    Row(id=2818,exp=5),\n",
    "    Row(id=10787,exp=7)\n",
    "]\n",
    "small_df=spark.createDataFrame(data)\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a334d297-e90e-4c9f-b99e-39e7b483d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+-----------------+-----------------+---------+-----------+------+-----------------+----------+---------+--------------------+--------------------+----------+------------------+---------+--------+---+\n",
      "|   id|              name|    full_name|             address|         json_string|dob_string_format|score| department|salary|        hobbies_list|user_id|            txn_date|        txn_value|           street|     city|      state|   pin|     clean_spaces|first_name|last_name|        full_address|           json_data|       dob|               age|   rating|category|exp|\n",
      "+-----+------------------+-------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+-----------------+-----------------+---------+-----------+------+-----------------+----------+---------+--------------------+--------------------+----------+------------------+---------+--------+---+\n",
      "| 2818|     Samaksh Chada| Abha Dhingra|072\\nSetty Marg, ...|{\"city\":\"Salem\",\"...|       1996-03-28|   95|Engineering| 51215|[Photography, Tre...|    319|2025-12-23 13:20:...|737.2872543856215|79/24, Barad Marg|Bengaluru|Uttarakhand|635180|     SamakshChada|   Samaksh|    Chada|79/24, Barad Marg...|     {Salem, 392556}|1996-03-28|29.764383561643836|Excellent|  Others|  5|\n",
      "|10787|Jeremiah Mukherjee|Jonathan Rege|H.No. 731\\nRastog...|{\"city\":\"Coimbato...|       1996-08-25|   71|    Finance| 43745|[Cooking, Gaming,...|    288|2025-12-01 13:20:...|384.7680457192822|73/40, Saraf Road|   Mumbai|    Gujarat|639033|JeremiahMukherjee|  Jeremiah|Mukherjee|73/40, Saraf Road...|{Coimbatore, 478324}|1996-08-25|29.353424657534248|     Good|  Others|  7|\n",
      "+-----+------------------+-------------+--------------------+--------------------+-----------------+-----+-----------+------+--------------------+-------+--------------------+-----------------+-----------------+---------+-----------+------+-----------------+----------+---------+--------------------+--------------------+----------+------------------+---------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df.join(broadcast(small_df), \"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1da6f-7f71-456b-a489-38118e98d8d5",
   "metadata": {},
   "source": [
    "This avoids expensive shuffles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e40805-bb4b-42a0-8f0c-b6c42f2bdac5",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 10. Best Practices People Rarely Teach\n",
    "\n",
    "    * Avoid using UDFs unless absolutely needed\n",
    "    \n",
    "    * Prefer built-in functions (they are optimized)\n",
    "    \n",
    "    * Always specify schema for production workloads\n",
    "    \n",
    "    * Filter early â†’ reduces data size\n",
    "    \n",
    "    * Avoid wide transformations if possible\n",
    "    \n",
    "    * Prefer Parquet over CSV\n",
    "    \n",
    "    * Cache only when necessary\n",
    "    \n",
    "    * Monitor job plans using Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27a27e-57a0-40e0-89ad-2ab408c87be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
