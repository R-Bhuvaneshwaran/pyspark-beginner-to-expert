{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458cdeac-1d5a-41ff-a4d5-26bb26bc11b8",
   "metadata": {},
   "source": [
    "This notebook covers:\n",
    "- SparkSession fundamentals  \n",
    "- SparkContext  \n",
    "- Spark engine concepts  \n",
    "- RDD creation  \n",
    "- Transformations vs actions  \n",
    "- Lazy evaluation  \n",
    "- Narrow vs wide dependencies  \n",
    "- Hands-on examples  \n",
    "ðŸ“Œ This notebook is part of my \"PySpark Series: Beginner to Expert\".\n",
    "\n",
    "Medium blog: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f95f7a-8e33-48fd-a034-85d3d6b01499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758f173-8d32-42ec-91e6-783fb178d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"TestApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8d3b9-4ee1-456d-82b3-2870b1fd7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369cb3e-7945-4b7f-b8bb-a2893a03bea6",
   "metadata": {},
   "source": [
    "## âœ” What each line means:\n",
    "**from pyspark.sql import SparkSession**\n",
    "\n",
    "Imports the main entry point for PySpark DataFrame API.\n",
    "\n",
    "**SparkSession.builder**\n",
    "\n",
    "Starts the configuration builder.\n",
    "Think of it as filling a form before Spark starts.\n",
    "\n",
    "**.appName(\"TestApp\")**\n",
    "\n",
    "Names your application.\n",
    "\n",
    "Why name?\n",
    "\n",
    "Shows up in Spark UI\n",
    "\n",
    "Helps monitor jobs\n",
    "\n",
    "Helps in cluster logs\n",
    "\n",
    "**.getOrCreate()**\n",
    "\n",
    "This is important.\n",
    "\n",
    "If Spark is not running â†’ create it\n",
    "\n",
    "If SparkSession already exists â†’ return existing one\n",
    "\n",
    "This avoids multiple Spark sessions (which can crash JVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd8261-92c0-4376-8860-81c487fb1b9c",
   "metadata": {},
   "source": [
    "# âœ” Increase memory\n",
    "\n",
    ".config(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    ".config(\"spark.driver.memory\", \"2g\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be9fea-fb15-42c8-b169-3c0221dd82dc",
   "metadata": {},
   "source": [
    "# âœ” Enable Arrow for faster Pandas conversions\n",
    "\n",
    ".config(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f681f-e0bc-4a8c-836b-5e71725bf568",
   "metadata": {},
   "source": [
    "# âœ” Complete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019aeebf-6264-43ab-b4c0-239c75c5b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MySparkApp\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb271c8c-a7fd-4c1a-852a-7cb918cf98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70df60f-f714-409a-9289-a0f7d8bfa303",
   "metadata": {},
   "source": [
    "# ðŸ”¥ What Is SparkContext?\n",
    "\n",
    "* SparkContext = the â€œengine operatorâ€.\n",
    "\n",
    "* Manages the connection to the cluster\n",
    "\n",
    "* Transfers tasks to executors\n",
    "\n",
    "* Keeps track of job status\n",
    "\n",
    "* Handles scheduling\n",
    "\n",
    "You can access it through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49840b8d-57eb-4681-bde1-3fa765bd3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5d469-39c8-4d14-bd5b-667ea318fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.appName)\n",
    "print(sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d8f2a-c8cf-4d97-b1c5-2c6d0322c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "img = Image.open(\"Spark_Master_Types.png\")\n",
    "\n",
    "# Show image\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9825c9-50a1-425f-8409-e55128495e35",
   "metadata": {},
   "source": [
    "# ðŸ“‚ Reading Data with Spark (From Your Notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18790988-583f-4399-ba2c-7816e7d8e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"/home/bhuvaneshwaran/Desktop/Medium/csvFiles/Employee.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114cbc1-90ae-4c28-8406-d448476f33dc",
   "metadata": {},
   "source": [
    "### âœ” Explanation of each parameter:\n",
    "ðŸ· header=True\n",
    "\n",
    "Use the first row as column names\n",
    "Without header â†’ Spark names columns _c0, _c1, _c2...\n",
    "\n",
    "ðŸ“ inferSchema=True\n",
    "\n",
    "Spark automatically detects data types:\n",
    "\n",
    "1. integers\n",
    "\n",
    "2. strings\n",
    "\n",
    "3. floats\n",
    "\n",
    "4. timestamps\n",
    "\n",
    "If False, all columns become string.\n",
    "\n",
    "### â— Should you always use inferSchema?\n",
    "\n",
    "No.\n",
    "\n",
    "For production â†’ define schema manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51d40c-68ac-40d5-8ae1-fd945844db3e",
   "metadata": {},
   "source": [
    "## âœ” Alternatives to CSV\n",
    "* spark.read.json()\n",
    "\n",
    "* spark.read.parquet()\n",
    "\n",
    "* spark.read.orc()\n",
    "\n",
    "* spark.read.text()\n",
    "\n",
    "* spark.read.jdbc()\n",
    "\n",
    "**Parquet** is recommended for performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436696b-eb4c-4a89-bc6e-e3bcb2246d6f",
   "metadata": {},
   "source": [
    "## ðŸ›‘ Stopping Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90070437-1d78-4e91-b24c-8ba9afbffa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85314670-21e6-4101-9f81-82e82d74d0bb",
   "metadata": {},
   "source": [
    "Good practice because:\n",
    "\n",
    "* Releases memory\n",
    "\n",
    "* Stops JVM backend\n",
    "\n",
    "* Avoids â€œmultiple SparkContextâ€ errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f176bc-b657-48ab-89f4-11f6e59a4191",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Summary of What We Covered\n",
    "\n",
    "* What SparkSession is\n",
    "\n",
    "* Why itâ€™s the entry point\n",
    "\n",
    "* How .builder works\n",
    "\n",
    "* Why appName, master, configs matter\n",
    "\n",
    "* What SparkContext does\n",
    "\n",
    "* How clusters are chosen\n",
    "\n",
    "* Reading CSV with real examples\n",
    "\n",
    "* Explanation of every parameter\n",
    "\n",
    "* Best practices + alternatives\n",
    "\n",
    "This is an excellent foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7b355-3360-429e-b51d-592c773ac4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e57da1-4740-441e-8787-7721f7224594",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Creating RDDs in PySpark\n",
    "âœ” Method 1: parallelize (convert Python list â†’ RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac7a2a-0f23-4ecf-a3eb-8586e1e8c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca89785-f1ff-406d-94f0-fa437f95149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97159a-9f6e-4feb-85e8-938f3f78bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b845f5a-dde9-4412-916f-3dd6bfaee467",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5ece8-2fc0-484b-9f9c-d557627c9e73",
   "metadata": {},
   "source": [
    "When to use:\n",
    "\n",
    "* For testing\n",
    "\n",
    "* Small manual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc10e4d-ebc9-4f00-b7e9-c7a17a33dcc0",
   "metadata": {},
   "source": [
    "âœ” Method 2: Load from external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54542a30-c46b-4c08-9346-baf6841d9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(r\"/home/bhuvaneshwaran/Desktop/Medium/txtFiles/simple.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98badc2-20b5-47b1-9d86-0870f5dd382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8fd19-5ab2-40a0-9af9-2d137c341916",
   "metadata": {},
   "source": [
    "### When to use:\n",
    "\n",
    "* Large dataset\n",
    "\n",
    "* Streaming pipelines\n",
    "\n",
    "* Log processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cedd01-559c-4fd8-ab53-3b0fb9318cb3",
   "metadata": {},
   "source": [
    "## âš¡ Transformations vs Actions (Core Spark Concept)\n",
    "\n",
    "RDD operations are split into two types:\n",
    "\n",
    "## ðŸ”µ Transformations:\n",
    "\n",
    "These define the computation\n",
    "\n",
    "**(MAP, FILTER, FLATMAP, REDUCEBYKEY)**\n",
    "\n",
    "But they donâ€™t run immediately.\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225c95c-04b6-48d6-bc0a-de62ebd8c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: x*2)\n",
    "rdd.filter(lambda x: x > 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6d58b-4b7c-4035-aee5-bd432a2117de",
   "metadata": {},
   "source": [
    "These are lazy â€” Spark waits until it sees an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd265b8-2b58-49bc-a12d-333cec00d4eb",
   "metadata": {},
   "source": [
    "## ðŸ”´ Actions:\n",
    "\n",
    "These trigger the computation\n",
    "They tell Spark: â€œOkay, now give me the result.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fe054-22ac-41ba-82d0-a4226b96d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14593806-fd65-4470-bcc5-bfed48e31479",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b6e84-a008-4545-8b76-41f95e9ee40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e6c19-06e6-433b-a213-4f166147b9c0",
   "metadata": {},
   "source": [
    "## ðŸ§  Understanding Lazy Evaluation (Beginner-Friendly Story)\n",
    "\n",
    "Imagine writing steps:\n",
    "\n",
    "* wash vegetables\n",
    "\n",
    "* cut vegetables\n",
    "\n",
    "* cook vegetables\n",
    "\n",
    "But you start only when someone says:\n",
    "\n",
    "**â€œServe the food!â€**\n",
    "\n",
    "That's how **Spark** works.\n",
    "\n",
    "You tell Spark the steps (transformations).\n",
    "Spark waits.\n",
    "When an action arrives â†’ it runs the entire chain.\n",
    "\n",
    "Result:\n",
    "\n",
    "* Optimized execution\n",
    "\n",
    "* Better performance\n",
    "\n",
    "* Fewer unnecessary operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac13f44-bd3f-4ccc-99bf-b870051575b0",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Real Example: Transformations + Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f8d0d-ea31-4ae7-9144-0ea43314ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "transformed = rdd \\\n",
    "    .filter(lambda x: x % 2 == 0) \\\n",
    "    .map(lambda x: x * 10)\n",
    "\n",
    "result = transformed.collect()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a56cf7-b43f-4ad1-9b71-4be7cf37d206",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "**filter** â†’ keep even numbers\n",
    "\n",
    "**map** â†’ multiply by 10\n",
    "\n",
    "**collect** â†’ triggers execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2323ac4-b012-417e-bb43-14657756d7bb",
   "metadata": {},
   "source": [
    "## ðŸŒ‰ Narrow vs Wide Transformations\n",
    "\n",
    "This is SUPER important for performance.\n",
    "\n",
    "## ðŸ”µ Narrow Transformation\n",
    "\n",
    "Data stays on the same partition:\n",
    "\n",
    "Examples:\n",
    "\n",
    "* map\n",
    "\n",
    "* filter\n",
    "\n",
    "* flatMap\n",
    "\n",
    "Fast because NO SHUFFLE.\n",
    "\n",
    "## ðŸ”´ Wide Transformation\n",
    "\n",
    "Data needs to move across partitions â†’ SHUFFLE\n",
    "\n",
    "Examples:\n",
    "\n",
    "* reduceByKey\n",
    "\n",
    "* groupBy\n",
    "\n",
    "* join\n",
    "\n",
    "This is expensive and slow.\n",
    "\n",
    "âœ” Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d77cb-b146-4ba6-8e77-520193c33d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c96a85-f1ce-4cfe-a041-5a43cd03af7b",
   "metadata": {},
   "source": [
    "This requires grouping keys â†’ shuffle â†’ slower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6ecc4-1181-43da-9680-3ef4d80d72a6",
   "metadata": {},
   "source": [
    "## ðŸ’¡ RDD Alternatives\n",
    "âœ” Modern alternative â†’ DataFrames\n",
    "\n",
    "Why better?\n",
    "\n",
    "* Catalyst optimizer\n",
    "\n",
    "* Columnar processing\n",
    "\n",
    "* Faster execution\n",
    "\n",
    "* Easier syntax\n",
    "\n",
    "Example:\n",
    "\n",
    "    df.filter(df.age > 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a7405-281c-4e54-bb84-c14074f2b097",
   "metadata": {},
   "source": [
    "## âœ” Pandas API on Spark (pandas-on-spark)\n",
    "\n",
    "Best for pandas users transitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb5083-02b7-4dfa-ab8e-500bec1860d0",
   "metadata": {},
   "source": [
    "| Mistake                        | Why wrong             |\n",
    "| ------------------------------ | --------------------- |\n",
    "| Using `collect()` on huge data | Causes memory crash   |\n",
    "| Too many wide transformations  | Slow jobs             |\n",
    "| Creating many small partitions | Overhead              |\n",
    "| Using RDD everywhere           | DataFrames are faster |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ce531-6471-4267-80c5-6372070cac51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
